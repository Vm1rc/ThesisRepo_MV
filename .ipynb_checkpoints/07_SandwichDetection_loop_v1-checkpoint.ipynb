{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c40518-2b98-4ecb-b2bf-a29994dc7fbf",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4837411c-2303-4f85-b1c5-17d02dc93e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_colwidth', None) # show full width of showing cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b774ef29-d42d-427a-ae06-d7b2c768eb92",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240f9c7-5ac2-40a3-bac9-b0b664870312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_identifier(df):\n",
    "    str_id = df.apply(lambda x: '_'.join(map(str, x)), axis=1)\n",
    "    return pd.factorize(str_id)[0]\n",
    "# Apply like:\n",
    "# dftestMaster['UID_test'] = make_identifier(dftestMaster[['BatchNr','attack_Id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7236817d-118f-4c6d-a07e-61ff46662041",
   "metadata": {},
   "source": [
    "# Set base variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd979b-6ece-4be9-80fe-5c410a7c916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "counter_save = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7672076-ae8e-4231-99b1-9e8ab18f6b6d",
   "metadata": {},
   "source": [
    "# Load new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a076b-e358-4909-9836-04b0c5572c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_parquet(f\"/Volumes/Extreme SSD/02_UniswapV2Transactions/UniswapV2Transactions_{counter}.par\")\n",
    "counter = counter + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8033d8ba-afca-48dd-92fb-6de236842553",
   "metadata": {},
   "source": [
    "# Convert data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8041b5a6-7980-4283-91f6-361b2ec51e64",
   "metadata": {},
   "source": [
    "## Convert gasprice and Value to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c304ab-0609-4594-8b9d-7ec3a1a721f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['trans_gasPrice_Gwei'] = (df1['trans_gasPrice']/(10**9))\n",
    "df1['value_float'] = (df1['value'].astype(float)/(10**18)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f3fb5-5526-434b-ae0d-4a253517e6fb",
   "metadata": {},
   "source": [
    "# Build loop (1 observation per transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18480896-94e7-4be6-9dba-2a6ec6b43400",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "counter_save = 1\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Add control ouputs\n",
    "    print(counter,\n",
    "          counter_save)\n",
    "    \n",
    "    try:\n",
    "        df1 = pd.read_parquet(f\"/Volumes/Extreme SSD/02_UniswapV2Transactions/UniswapV2Transactions_{counter}.par\")\n",
    "    except:\n",
    "        print(\"Finished!\")\n",
    "        break           \n",
    "    else:\n",
    "        # Convert gasprice and Value to float\n",
    "        df1['trans_gasPrice_Gwei'] = (df1['trans_gasPrice']/(10**9))\n",
    "        df1['value_float'] = (df1['value'].astype(float)/(10**18))\n",
    "        \n",
    "        # Heuristics 1,4,6a\n",
    "        df1['Count_1']=df1.groupby(['block_number','poolId', 'value'],as_index=True)['value'].transform('size').astype('float') \n",
    "\n",
    "        df_filtered = df1.groupby(['block_number','poolId', 'transaction_hash'],as_index=True).filter(lambda x: x['Count_1'].mean() > 1)\n",
    "        \n",
    "        # Heuristics 3 & 7\n",
    "        # Add column with unique count of transaction_hash per sub-frame\n",
    "        df_filtered['Count_nHash']=df_filtered.groupby(['block_number','poolId', 'value'],as_index=True)['transaction_hash'].transform('nunique').astype('float')\n",
    "\n",
    "        # filtering 'Count_nHash'-mean == 1.5 (formerly >1) per sub-frame by 'transaction_hash' means that there are exact 2 (fromaerly at least 2) different transaction_hashes, which is heuristic nr. 3\n",
    "        df_filtered2 = df_filtered.groupby(['transaction_hash'],as_index=True).filter(lambda x: x['Count_nHash'].mean()  == 1.5)\n",
    "        \n",
    "        # Heuristic 5\n",
    "        df_filtered2['within_transaction_order'] = df_filtered2.sort_values(['log_index'],ascending=True).groupby(['transaction_hash'],as_index=True).cumcount().add(1)\n",
    "\n",
    "\n",
    "        # filtering for mean values == 1.5 (formerly >= 1.5) for within_transaction_order after groupby to ensure Heuristic 5 (transaction in oposite transaction per block, LP, and token_address\n",
    "        df_filtered3 = df_filtered2[df_filtered2.duplicated(subset=['block_number', 'poolId', 'tf_tokenAddress', 'value'],keep = False)].groupby(['block_number', 'poolId', 'tf_tokenAddress', 'value'],as_index=True).filter(lambda x: x['within_transaction_order'].mean() == 1.5)\n",
    "\n",
    "        # Adding an attack-ID\n",
    "        df_filtered3['attack_Id'] = df_filtered3.groupby(['block_number', 'poolId', 'value'],as_index=True).ngroup()\n",
    "        \n",
    "        # Adding batch number\n",
    "        df_filtered3['BatchNr'] = counter\n",
    "\n",
    "              \n",
    "        \n",
    "    \n",
    "    # Check if first loop\n",
    "    if os.path.exists(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_Master.par\"):\n",
    "        # Save seperate file with counter number (100k blocks)\n",
    "        df_filtered3.to_parquet(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_{counter_save}.par\")\n",
    "    \n",
    "        # Add to Master file\n",
    "        dfMaster = pd.read_parquet(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_Master.par\")\n",
    "        dfMaster = pd.concat([dfMaster, df_filtered3], ignore_index=True)\n",
    "        # Add overall attack Id\n",
    "        dfMaster['Attack_UID'] = dfMaster.groupby(['BatchNr','attack_Id']).ngroup()\n",
    "        dfMaster.to_parquet(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_Master.par\")\n",
    "        \n",
    "        # Clean/update variables\n",
    "        counter = counter + 1\n",
    "        counter_save = counter_save + 1\n",
    "        del df1\n",
    "        del df_filtered\n",
    "        del df_filtered2\n",
    "        del df_filtered3\n",
    "        del dfMaster\n",
    "    \n",
    "\n",
    "    else:\n",
    "        \n",
    "        dfMaster = df_filtered3\n",
    "        # Add overall attack Id\n",
    "        dfMaster['Attack_UID'] = dfMaster.groupby(['BatchNr','attack_Id']).ngroup()\n",
    "        # Save Master\n",
    "        df_filtered3.to_parquet(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_Master.par\")\n",
    "        # Save first file\n",
    "        df_filtered3.to_parquet(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_{counter_save}.par\")\n",
    "        \n",
    "        # Clean/update variables\n",
    "        counter = counter + 1\n",
    "        counter_save = counter_save + 1\n",
    "        del df1\n",
    "        del df_filtered\n",
    "        del df_filtered2\n",
    "        del df_filtered3\n",
    "        del dfMaster\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc6858-38cf-4fc2-986c-de86c61fff38",
   "metadata": {},
   "source": [
    "# Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962b581-d582-418a-98c4-8c6bbe61009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest1 = pd.read_parquet(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_1.par\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9a284-e28f-4ba4-a2d8-e00314650f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest2 = pd.read_parquet(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_3.par\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b5162-0e7f-49d1-aace-0fb227b4f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftestMaster = pd.read_parquet(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_Master.par\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dea530-7125-401e-81e4-8e2da0303f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dftest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f840a-2e07-4e6c-ae7f-7bd6a67ee8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dftest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a2ff65-0cfd-4a6d-a8da-e6842fc85aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dftestMaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020fb4a1-6c32-40f1-957e-bf1d3d9d998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftestMaster[dftestMaster['BatchNr'] ==18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e0dde5-9104-45f7-8e77-40fa51593b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftestMaster['Attack_UID'] = dftestMaster.groupby(['BatchNr','attack_Id']).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b7539-6ee5-4f97-9a5f-93711de568f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftestMaster = dftestMaster.drop(['UID','UID_test'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeef0b8-5469-48e7-bc5f-a56a88011a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftestMaster.to_parquet(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_Master.par\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57ec84-7dc1-4835-b4ed-afe4613df999",
   "metadata": {},
   "source": [
    "# Build loop (ALL observations per transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef726a-28dd-490a-92a2-11e318054b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "counter_save = 1\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Add control ouputs\n",
    "    print(counter,\n",
    "          counter_save)\n",
    "    \n",
    "    try:\n",
    "        df1 = pd.read_parquet(f\"/Volumes/Extreme SSD/02_UniswapV2Transactions/UniswapV2Transactions_{counter}.par\")\n",
    "    except:\n",
    "        print(\"Finished!\")\n",
    "        break           \n",
    "    else:\n",
    "        # Convert gasprice and Value to float\n",
    "        df1['trans_gasPrice_Gwei'] = (df1['trans_gasPrice']/(10**9))\n",
    "        df1['value_float'] = (df1['value'].astype(float)/(10**18))\n",
    "        \n",
    "        # Heuristics 1,4,6\n",
    "        df1['Count_1']=df1.groupby(['block_number','poolId', 'value'],as_index=True)['value'].transform('size').astype('float') \n",
    "\n",
    "        df_filtered = df1.groupby(['block_number','poolId', 'transaction_hash'],as_index=True).filter(lambda x: x['Count_1'].mean() > 1)\n",
    "        \n",
    "        # Heuristics 3 & 7\n",
    "        # Add column with unique count of transaction_hash per sub-frame\n",
    "        df_filtered['Count_nHash']=df_filtered.groupby(['block_number','poolId', 'value'],as_index=True)['transaction_hash'].transform('nunique').astype('float')\n",
    "\n",
    "        # filtering 'Count_nHash'-mean == 1.5 (formerly >1) per sub-frame by 'transaction_hash' means that there are exact 2 (fromaerly at least 2) different transaction_hashes, which is heuristic nr. 3\n",
    "        df_filtered2 = df_filtered.groupby(['transaction_hash'],as_index=True).filter(lambda x: x['Count_nHash'].mean()  == 1.5)\n",
    "        \n",
    "        # Heuristic 5\n",
    "        df_filtered2['within_transaction_order'] = df_filtered2.sort_values(['log_index'],ascending=True).groupby(['transaction_hash'],as_index=True).cumcount().add(1)\n",
    "        \n",
    "        AttackHashes = df_filtered2[df_filtered2.duplicated(subset=['block_number', 'poolId', 'tf_tokenAddress', 'value'],keep = False)].groupby(['block_number', 'poolId', 'tf_tokenAddress', 'value'],as_index=True).filter(lambda x: x['within_transaction_order'].mean() == 1.5)\n",
    "        AttackHashes = AttackHashes['transaction_hash'].unique()\n",
    "        df_filtered3 = df_filtered2[df_filtered2['transaction_hash'].isin(AttackHashes)]\n",
    "        \n",
    "        ##### Adding an attack-ID\n",
    "        # Adding an attack-ID to df with 1 observation per transaction\n",
    "        df_attacksSingleObs = df_filtered2[df_filtered2.duplicated(subset=['block_number', 'poolId', 'tf_tokenAddress', 'value'],keep = False)].groupby(['block_number', 'poolId', 'tf_tokenAddress', 'value'],as_index=True).filter(lambda x: x['within_transaction_order'].mean() == 1.5)\n",
    "        df_attacksSingleObs['attack_Id'] = df_attacksSingleObs.groupby(['block_number', 'poolId', 'value'],as_index=True).ngroup()\n",
    "        \n",
    "        # Merge with full df\n",
    "        df_filtered3 = pd.merge(df_filtered3,df_attacksSingleObs[['transaction_hash','attack_Id']],on='transaction_hash', how='left').drop_duplicates()\n",
    "\n",
    "        \n",
    "        # Adding batch number\n",
    "        df_filtered3['BatchNr'] = counter\n",
    "\n",
    "              \n",
    "        \n",
    "    \n",
    "    # Check if first loop\n",
    "    if os.path.exists(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_full_Master.par\"):\n",
    "        # Save seperate file with counter number (100k blocks)\n",
    "        df_filtered3.to_parquet(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_full_{counter_save}.par\")\n",
    "    \n",
    "        # Add to Master file\n",
    "        dfMaster = pd.read_parquet(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_full_Master.par\")\n",
    "        dfMaster = pd.concat([dfMaster, df_filtered3], ignore_index=True)\n",
    "        # Add overall attack Id\n",
    "        dfMaster['Attack_UID'] = dfMaster.groupby(['BatchNr','attack_Id']).ngroup()\n",
    "        dfMaster.to_parquet(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_full_Master.par\")\n",
    "        \n",
    "        # Clean/update variables\n",
    "        counter = counter + 1\n",
    "        counter_save = counter_save + 1\n",
    "        del df1\n",
    "        del df_filtered\n",
    "        del df_filtered2\n",
    "        del df_filtered3\n",
    "        del dfMaster\n",
    "        del AttackHashes\n",
    "        del df_attacksSingleObs\n",
    "    \n",
    "\n",
    "    else:\n",
    "        \n",
    "        dfMaster = df_filtered3\n",
    "        # Add overall attack Id\n",
    "        dfMaster['Attack_UID'] = dfMaster.groupby(['BatchNr','attack_Id']).ngroup()\n",
    "        # Save Master\n",
    "        df_filtered3.to_parquet(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_full_Master.par\")\n",
    "        # Save first file\n",
    "        df_filtered3.to_parquet(f\"/Volumes/Extreme SSD/98_Output/AttacksUniswapV2_full_{counter_save}.par\")\n",
    "        \n",
    "        # Clean/update variables\n",
    "        counter = counter + 1\n",
    "        counter_save = counter_save + 1\n",
    "        del df1\n",
    "        del df_filtered\n",
    "        del df_filtered2\n",
    "        del df_filtered3\n",
    "        del dfMaster\n",
    "        del AttackHashes\n",
    "        del df_attacksSingleObs\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e84fa-3141-48d6-aeb0-ab0bfb1e941b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633513d9-8476-4412-9608-607933f97cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
