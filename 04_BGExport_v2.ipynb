{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Useful Links\n",
    "Useful links:\n",
    "* Export job to GCS: https://blog.getcensus.com/3-ways-to-export-csv-files-from-google-bigquery/\n",
    "* Export job to GCS: https://hevodata.com/learn/export-bigquery-table-to-csv/#met3\n",
    "* Export job to GCS: https://cloud.google.com/bigquery/docs/samples/bigquery-extract-table#bigquery_extract_table-python\n",
    "* Query BQ via Python: https://sophiamyang.github.io/DS/dataaccess/bigquery/bigquery.html\n",
    "* Query via Python: https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries?hl=de#run_the_query\n",
    "\n",
    "* Query GBQ with pandas.gbq: https://stackoverflow.com/questions/44647310/convert-bigquery-results-to-pandas-data-frame\n",
    "* Doc pandas-gbq: https://pandas-gbq.readthedocs.io/en/latest/install.html#conda\n",
    "* Google doc zu pandas-gbq: https://cloud.google.com/bigquery/docs/pandas-gbq-migration?hl=de#conda\n",
    "* Running Parametrized queries: https://cloud.google.com/bigquery/docs/parameterized-queries\n",
    "* .to_dataframe doc: https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html#google.cloud.bigquery.job.QueryJob.to_dataframe\n",
    "* Get BQ table as dataframe: https://cloud.google.com/bigquery/docs/samples/bigquery-list-rows-dataframe\n",
    "* globals().clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery # when buggy: 'conda remove protobuf'\n",
    "import os\n",
    "import pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "print(bigquery.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set env variable for credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/Users/marcvendramet/Desktop/HSG/Master/MA/08_Data/03_BQKey/mevtest-32e3980294f1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Query Masterset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query = '''\n",
    "SELECT *\n",
    "\n",
    "\n",
    "FROM `mevtest.MEVextract.UniswapV2_transactions_download_v4`\n",
    "WHERE block_number BETWEEN  @lower_bound AND @upper_bound \n",
    "\n",
    "ORDER BY block_number ASC, transaction_index ASC, log_index ASC\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqCols = ['transaction_hash','block_number','transaction_index','log_index','from_address','to_address',\n",
    "                            'trans_fromAddress','trans_toAddress', 'poolId', 'tf_tokenAddress', 'tokenName',\n",
    "                            'value','trans_gasPrice','gasUsed','effective_gas_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMain = pd.DataFrame(columns = bqCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "lowerB = 10093070\n",
    "upperB = lowerB + 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [76]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m job_config1 \u001b[38;5;241m=\u001b[39m bigquery\u001b[38;5;241m.\u001b[39mQueryJobConfig(\n\u001b[1;32m      2\u001b[0m query_parameters\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      3\u001b[0m         bigquery\u001b[38;5;241m.\u001b[39mScalarQueryParameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlower_bound\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINT64\u001b[39m\u001b[38;5;124m\"\u001b[39m, lowerB),\n\u001b[1;32m      4\u001b[0m         bigquery\u001b[38;5;241m.\u001b[39mScalarQueryParameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper_bound\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINT64\u001b[39m\u001b[38;5;124m\"\u001b[39m, upperB),\n\u001b[1;32m      5\u001b[0m     ]\n\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      8\u001b[0m query_job \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mquery(my_query, job_config\u001b[38;5;241m=\u001b[39mjob_config1)\n\u001b[0;32m---> 10\u001b[0m dfTemp \u001b[38;5;241m=\u001b[39m \u001b[43mquery_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# create path variable\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# save to.csv or parquet on ssd OK\u001b[39;00m\n\u001b[1;32m     13\u001b[0m dfTemp\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Volumes/Extreme SSD/02_UniswapV2Transactions/UniswapV2Transactions_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcounter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.par\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Thesis39/lib/python3.9/site-packages/google/cloud/bigquery/job/query.py:1684\u001b[0m, in \u001b[0;36mQueryJob.to_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object)\u001b[0m\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;124;03m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[1;32m   1618\u001b[0m \n\u001b[1;32m   1619\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;124;03m        :mod:`shapely` library cannot be imported.\u001b[39;00m\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1683\u001b[0m query_result \u001b[38;5;241m=\u001b[39m wait_for_query(\u001b[38;5;28mself\u001b[39m, progress_bar_type, max_results\u001b[38;5;241m=\u001b[39mmax_results)\n\u001b[0;32m-> 1684\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbqstorage_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbqstorage_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_bqstorage_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_bqstorage_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeography_as_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeography_as_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Thesis39/lib/python3.9/site-packages/google/cloud/bigquery/table.py:1939\u001b[0m, in \u001b[0;36mRowIterator.to_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, geography_as_object)\u001b[0m\n\u001b[1;32m   1936\u001b[0m     create_bqstorage_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     bqstorage_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1939\u001b[0m record_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arrow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbqstorage_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbqstorage_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_bqstorage_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_bqstorage_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1945\u001b[0m \u001b[38;5;66;03m# When converting date or timestamp values to nanosecond precision, the result\u001b[39;00m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# can be out of pyarrow bounds. To avoid the error when converting to\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# Pandas, we set the date_as_object or timestamp_as_object parameter to True,\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# if necessary.\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m date_as_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m   1950\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__can_cast_timestamp_ns(col)\n\u001b[1;32m   1951\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m record_batch\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(col\u001b[38;5;241m.\u001b[39mtype)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1955\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Thesis39/lib/python3.9/site-packages/google/cloud/bigquery/table.py:1742\u001b[0m, in \u001b[0;36mRowIterator.to_arrow\u001b[0;34m(self, progress_bar_type, bqstorage_client, create_bqstorage_client)\u001b[0m\n\u001b[1;32m   1737\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m get_progress_bar(\n\u001b[1;32m   1738\u001b[0m     progress_bar_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_rows, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1739\u001b[0m )\n\u001b[1;32m   1741\u001b[0m record_batches \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1742\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m record_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_arrow_iterable(\n\u001b[1;32m   1743\u001b[0m     bqstorage_client\u001b[38;5;241m=\u001b[39mbqstorage_client\n\u001b[1;32m   1744\u001b[0m ):\n\u001b[1;32m   1745\u001b[0m     record_batches\u001b[38;5;241m.\u001b[39mappend(record_batch)\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m progress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;66;03m# In some cases, the number of total rows is not populated\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;66;03m# until the first page of rows is fetched. Update the\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;66;03m# progress bar's total to keep an accurate count.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Thesis39/lib/python3.9/site-packages/google/cloud/bigquery/table.py:1612\u001b[0m, in \u001b[0;36mRowIterator._to_page_iterable\u001b[0;34m(self, bqstorage_download, tabledata_list_download, bqstorage_client)\u001b[0m\n\u001b[1;32m   1605\u001b[0m     bqstorage_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1607\u001b[0m result_pages \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1608\u001b[0m     bqstorage_download()\n\u001b[1;32m   1609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bqstorage_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1610\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m tabledata_list_download()\n\u001b[1;32m   1611\u001b[0m )\n\u001b[0;32m-> 1612\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m result_pages\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Thesis39/lib/python3.9/site-packages/google/cloud/bigquery/_pandas_helpers.py:906\u001b[0m, in \u001b[0;36m_download_table_bqstorage\u001b[0;34m(project_id, table, bqstorage_client, preserve_order, selected_fields, page_to_item, max_queue_size)\u001b[0m\n\u001b[1;32m    903\u001b[0m     future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 906\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mworker_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_PROGRESS_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m frame\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Thesis39/lib/python3.9/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Thesis39/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "job_config1 = bigquery.QueryJobConfig(\n",
    "query_parameters=[\n",
    "        bigquery.ScalarQueryParameter(\"lower_bound\", \"INT64\", lowerB),\n",
    "        bigquery.ScalarQueryParameter(\"upper_bound\", \"INT64\", upperB),\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "query_job = client.query(my_query, job_config=job_config1)\n",
    "    \n",
    "dfTemp = query_job.to_dataframe()\n",
    "# create path variable\n",
    "# save to.csv or parquet on ssd OK\n",
    "dfTemp.to_parquet(f\"/Volumes/Extreme SSD/02_UniswapV2Transactions/UniswapV2Transactions_{counter}.par\")\n",
    "# collect params from dfTemp andd update counter for new query OK\n",
    "counter = counter + 1\n",
    "lastEntry = dfTemp[\"block_number\"].max().item() # OR: dfMain[\"block_number\"].iloc[-1]\n",
    "lowerB = lastEntry+1\n",
    "upperB = lastEntry+100001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10393072 10493072 4\n"
     ]
    }
   ],
   "source": [
    "len(dfTemp.iloc[:,0]) >=1\n",
    "print(lowerB, upperB, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTemp.to_csv(f\"/Volumes/Extreme SSD/02_UniswapV2Transactions/UniswapV2Transactions_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTemp1 = dfTemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastEntry= lastEntry.item()\n",
    "lowerB=lowerB.item()\n",
    "upperB=upperB.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lastEntry.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic loop (full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10093070 10193070\n",
      "10193070\n",
      "10193071 10293071\n",
      "10293070\n",
      "10293071 10393071\n",
      "10393071\n",
      "10393072 10493072\n",
      "10493072\n",
      "10493073 10593073\n",
      "10593073\n",
      "10593074 10693074\n",
      "10693074\n",
      "10693075 10793075\n",
      "10793075\n",
      "10793076 10893076\n",
      "10893076\n",
      "10893077 10993077\n",
      "10993077\n",
      "10993078 11093078\n",
      "11093078\n",
      "11093079 11193079\n",
      "11193079\n",
      "11193080 11293080\n",
      "11293080\n",
      "11293081 11393081\n",
      "11393081\n",
      "11393082 11493082\n",
      "11493082\n",
      "11493083 11593083\n",
      "11593083\n",
      "11593084 11693084\n",
      "11693084\n",
      "11693085 11793085\n",
      "11793085\n",
      "11793086 11893086\n",
      "11893086\n",
      "11893087 11993087\n",
      "11993087\n",
      "11993088 12093088\n",
      "12093088\n",
      "12093089 12193089\n",
      "12193089\n",
      "12193090 12293090\n",
      "12293090\n",
      "12293091 12393091\n",
      "12393091\n",
      "12393092 12493092\n",
      "12493092\n",
      "12493093 12593093\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # len(dfTemp.iloc[:,0]) >=1\n",
    "    print(lowerB, upperB)\n",
    "    job_config1 = bigquery.QueryJobConfig(\n",
    "    query_parameters=[\n",
    "        bigquery.ScalarQueryParameter(\"lower_bound\", \"INT64\", lowerB),\n",
    "        bigquery.ScalarQueryParameter(\"upper_bound\", \"INT64\", upperB),\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    query_job = client.query(my_query, job_config=job_config1)\n",
    "    \n",
    "    dfTemp = query_job.to_dataframe()\n",
    "    \n",
    "    if dfTemp[\"block_number\"].max() >= 12500188:\n",
    "        dfTemp.to_parquet(f\"/Volumes/Extreme SSD/02_UniswapV2Transactions/UniswapV2Transactions_{counter}.par\")\n",
    "        break\n",
    "    else:\n",
    "        # create path variable\n",
    "        # save to.csv or parquet on ssd OK\n",
    "        dfTemp.to_parquet(f\"/Volumes/Extreme SSD/02_UniswapV2Transactions/UniswapV2Transactions_{counter}.par\")\n",
    "        # collect params from dfTemp andd update counter for new query OK\n",
    "        counter = counter + 1\n",
    "        lastEntry =dfTemp[\"block_number\"].max().item() # OR: dfMain[\"block_number\"].iloc[-1]\n",
    "        lowerB = lastEntry+1\n",
    "        upperB = lastEntry+100001\n",
    "        print(lastEntry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nice print outs of status, loop number, part of loop its in, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500188"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTemp['block_number'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229801, 15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTemp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query: GasPrice Threeshold data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query_GP = '''\n",
    "SELECT *  \n",
    "FROM `mevtest.MEVextract.GasPrice_threshold4`\n",
    "\n",
    "WHERE block_number BETWEEN  11296630 AND 12500188 \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1203560"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11296630-10093070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_job = client.query(my_query_GP)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTemp = query_job.to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500188"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTemp['block_number'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTemp.to_parquet(f\"/Volumes/Extreme SSD/01_GasData/GasPriceThreshold_02.par\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11296629.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (12500188-10093070)/2\n",
    "10093070 + a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive\n",
    "## Query function\n",
    "def query_mev():\n",
    "    #client = bigquery.Client()\n",
    "    query_job = client.query(\n",
    "    \"\"\"\n",
    "    SELECT * \n",
    "    FROM `mevtest.MEVextract.LPs_UniswapV2`\n",
    "    \"\"\")\n",
    "    #results = query_job.result()\n",
    "    #yield from results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential eeasy way to store as pandas df\n",
    "dfTest = query_job.to_dataframe() https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html#google.cloud.bigquery.job.QueryJob.to_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unpack function def (used before .to_dataframe())\n",
    "def query_unpack():\n",
    "    results = query_job.result()\n",
    "    yield from results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect results and add to df\n",
    "results2 = query_unpack()\n",
    "dftemp = []\n",
    "for row in results2:\n",
    "    dftemp.append([\n",
    "        row.block_number\n",
    "    ])\n",
    "\n",
    "dftemp = pd.DataFrame(dftemp, columns = ['block_number'])\n",
    "dfMain = dfMain.append(dftemp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query loop old version\n",
    "while True:\n",
    "    job_config1 = bigquery.QueryJobConfig(\n",
    "    query_parameters=[\n",
    "        bigquery.ScalarQueryParameter(\"lower_bound\", \"INT64\", lowerB),\n",
    "        bigquery.ScalarQueryParameter(\"upper_bound\", \"INT64\", upperB),\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    query_job = client.query(my_query, job_config=job_config1)\n",
    "    \n",
    "    dfTemp = query_job.to_dataframe()\n",
    "    \n",
    "    if len(dfTemp.iloc[:,0]) <=1:\n",
    "        break\n",
    "    else:\n",
    "        # create path variable\n",
    "        # save to.csv or parquet on ssd OK\n",
    "        dfTemp.to_parquet(f\"/Volumes/Extreme SSD/02_UniswapV2Transactions/UniswapV2Transactions_{counter}.par\")\n",
    "        # collect params from dfTemp andd update counter for new query OK\n",
    "        counter = counter + 1\n",
    "        lastEntry =dfTemp[\"block_number\"].max() # OR: dfMain[\"block_number\"].iloc[-1]\n",
    "        lowerB = lastEntry+1\n",
    "        upperB = lastEntry+100001\n",
    "        # check for max block\n",
    "        if dfTemp[\"block_number\"].max() >= 12500188:\n",
    "            break\n",
    "        else:\n",
    "            # clean dfTemp\n",
    "            dfTemp1 =dfTemp\n",
    "            dfTemp =pd.DataFrame()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
